import os
import re
import warnings
from typing import List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import google.generativeai as genai
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from sqlalchemy.exc import SQLAlchemyError
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util


# --- ì„¤ì • ---
ORIGINAL_ARTICLE_TABLE = "news_articles"
RESULT_TABLE_NAME = "article_topics"

GEMINI_MODEL_NAME = "gemini-2.5-flash"
TOPIC_CATEGORIES = [
    "ì‹ ì‚¬ì—…/M&A", "ê²½ì˜ì „ëµ/ë¦¬ë”ì‹­", "í•´ì™¸ì§„ì¶œ/ê¸€ë¡œë²Œ ë™í–¥", "íˆ¬ììœ ì¹˜/ì¬ë¬´", "ì‹ ì œí’ˆ/ì„œë¹„ìŠ¤ ì¶œì‹œ",
    "ê¸°ìˆ ê°œë°œ/R&D", "ìƒì‚°/ê³µê¸‰ë§ ê´€ë¦¬", "íŠ¹í—ˆ/ê¸°ìˆ ì¸ì¦", "ì‹œì¥ë™í–¥/íŠ¸ë Œë“œ ë¶„ì„", "ê²½ìŸì‚¬ ë™í–¥",
    "ì •ë¶€ê·œì œ/ì •ì±…", "ì¸ì¬ì±„ìš©/ì¸ì¬ìƒ", "ì¡°ì§ë¬¸í™”/ì¸ì‚¬ì œë„", "ì„ì§ì› ë™ì •/ì¸ì‚¬", "ë…¸ì‚¬ê´€ê³„/ê³ ìš©ì´ìŠˆ",
    "ESG/ì§€ì†ê°€ëŠ¥ê²½ì˜", "ì‚¬íšŒê³µí—Œ/CSR", "ì†Œë¹„ìë³´í˜¸/ë¶„ìŸ", "íŒŒíŠ¸ë„ˆì‹­/í˜‘ë ¥", "ëŒ€ì™¸í™œë™/í™ë³´", "ë¦¬ìŠ¤í¬/ìœ„ê¸°ê´€ë¦¬"
]
MAX_WORKERS = 5


# --- DB & AI ì„¤ì • ---
def setup_db_engine() -> Engine:
    load_dotenv()
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise ValueError("DATABASE_URLì´ .envì— ì—†ìŠµë‹ˆë‹¤.")
    return create_engine(db_url)


def setup_gemini() -> genai.GenerativeModel:
    warnings.filterwarnings("ignore")
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel(GEMINI_MODEL_NAME)


# --- DB í•¨ìˆ˜ ---
def fetch_all_unprocessed_articles(engine: Engine) -> Optional[pd.DataFrame]:
    print(f"\n1ë‹¨ê³„: '{RESULT_TABLE_NAME}'ì— ì—†ëŠ” ê¸°ì‚¬ ì¡°íšŒ ì¤‘...")

    query = text(f"""
        SELECT a.*
        FROM {ORIGINAL_ARTICLE_TABLE} a
        LEFT JOIN {RESULT_TABLE_NAME} ar ON a.article_id = ar.id
        WHERE ar.id IS NULL
        ORDER BY a.article_id ASC;
    """)

    try:
        with engine.connect() as conn:
            df = pd.read_sql_query(query, conn)
        df.dropna(subset=["content"], inplace=True)
        df.reset_index(drop=True, inplace=True)

        if df.empty:
            print("âœ… ì²˜ë¦¬í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        print(f"âœ… {len(df)}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ.")
        return df

    except SQLAlchemyError as e:
        print(f"âŒ DB ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None


def insert_analysis_report(engine: Engine, df: pd.DataFrame):
    print(f"\n4ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ë¥¼ '{RESULT_TABLE_NAME}'ì— ì €ì¥ ì¤‘...")

    try:
        df_insert = df[[
            "article_id", "company_id", "title", "published_at",
            "content", "url", "search_keyword", "cluster_id",
            "summary", "topic"
        ]].copy()

        df_insert.rename(columns={
            "article_id": "id",
            "company_id": "ê¸°ì—…ëª…",
            "published_at": "ì‘ì„±ì¼",
            "url": "ë§í¬",
            "search_keyword": "ë¹„ê³ "
        }, inplace=True)

        with engine.begin() as conn:
            df_insert.to_sql(RESULT_TABLE_NAME, con=conn, if_exists="append", index=False, chunksize=100)

        print(f"âœ… {len(df)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ.")

    except SQLAlchemyError as e:
        print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {e}")


# --- AI ë¶„ì„ ---
def get_summary_and_topic(content: str, model: genai.GenerativeModel, categories: List[str]) -> Tuple[str, str]:
    category_list = ", ".join(categories)
    prompt = (
        f"ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì½ê³  ë‘ ê°€ì§€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.\n"
        f"1. ìš”ì•½: ê¸°ì‚¬ ë‚´ìš©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°íˆ ìš”ì•½.\n"
        f"2. í† í”½: ì•„ë˜ ëª©ë¡ ì¤‘ ê°€ì¥ ì í•©í•œ í•˜ë‚˜ë¥¼ ì„ íƒ.\n\n"
        f"[í† í”½ ëª©ë¡]\n{category_list}\n\n"
        f"[ë³¸ë¬¸]\n{content}\n\n"
        f"---ì‘ë‹µ í˜•ì‹---\n"
        f"ìš”ì•½: ...\ní† í”½: ..."
    )

    try:
        res = model.generate_content(prompt)
        text = res.text.strip().replace("**", "")

        summary = re.search(r"ìš”ì•½:\s*(.*?)(?=\n?í† í”½:|\Z)", text, re.DOTALL)
        topic = re.search(r"í† í”½:\s*(.*)", text)

        summary_txt = summary.group(1).strip() if summary else "ìš”ì•½ ì‹¤íŒ¨"
        topic_txt = topic.group(1).strip() if topic else "ë¶„ë¥˜ ì‹¤íŒ¨"
        topic_clean = next((c for c in categories if c in topic_txt), "ë¶„ë¥˜ ì‹¤íŒ¨")

        return summary_txt, topic_clean

    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: {e}", "ë¶„ë¥˜ ì‹¤íŒ¨"


# --- í´ëŸ¬ìŠ¤í„°ë§ ---
def cluster_articles(df: pd.DataFrame) -> pd.DataFrame:
    print("\n2ë‹¨ê³„: ë³¸ë¬¸ ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
    if df.empty:
        df["cluster_id"] = np.nan
        return df

    try:
        model = SentenceTransformer("distiluse-base-multilingual-cased-v1")
        embeddings = model.encode(df["content"].tolist())
        clusters = util.community_detection(embeddings, min_community_size=2, threshold=0.8)
        cluster_map = {doc_id: i for i, cluster in enumerate(clusters) for doc_id in cluster}
        df["cluster_id"] = df.index.map(lambda x: cluster_map.get(x, -1))

        print(f"âœ… {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„° íƒì§€ ì™„ë£Œ.")
        return df

    except Exception as e:
        print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜: {e}")
        df["cluster_id"] = -1
        return df


# --- ë©”ì¸ ---
def run_topic_worker():
    print("--- InsightBee í† í”½ ë¶„ì„ ì‹œì‘ ---")

    try:
        engine = setup_db_engine()
        gemini_model = setup_gemini()
    except Exception as e:
        print(f"âŒ ì´ˆê¸° ì„¤ì • ì‹¤íŒ¨: {e}")
        return

    df_pending = fetch_all_unprocessed_articles(engine)
    if df_pending is None or df_pending.empty:
        return

    df_clustered = cluster_articles(df_pending)

    reps = df_clustered[df_clustered["cluster_id"] != -1].drop_duplicates(subset=["cluster_id"], keep="first")
    uniques = df_clustered[df_clustered["cluster_id"] == -1]
    df_target = pd.concat([reps, uniques]).sort_index()

    print(f"âœ… ë¶„ì„ ëŒ€ìƒ ê¸°ì‚¬ ìˆ˜: {len(df_target)}ê°œ")

    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(get_summary_and_topic, row["content"], gemini_model, TOPIC_CATEGORIES): row
            for _, row in df_target.iterrows()
        }

        for future in tqdm(as_completed(futures), total=len(df_target), desc="AI ë¶„ì„ ì¤‘"):
            row = futures[future]
            try:
                summary, topic = future.result()
            except Exception as e:
                summary, topic = f"API ì˜¤ë¥˜: {e}", "ë¶„ë¥˜ ì‹¤íŒ¨"

            row["summary"], row["topic"] = summary, topic
            results.append(row)

    df_final = pd.DataFrame(results)
    insert_analysis_report(engine, df_final)
    print("\nğŸ‰ ëª¨ë“  ê¸°ì‚¬ ë¶„ì„ ë° ì €ì¥ ì™„ë£Œ.")


if __name__ == "__main__":
    run_topic_worker()
